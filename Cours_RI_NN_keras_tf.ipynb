{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning\n",
    "Ce GitHub propose quelques exemples de réseaux de neurones assez simples. Illustrés à travers des exemples (collections de textes) (pris de la litérature: site de Keras ou Tensorflow). Les premiers exemples exloitet ds réseaux multicouhces. Si on arrive à e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premiers modèles multicouches"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Premier exemple d'utilisation Keras.\n",
    "Dans ce réseau milticouhces on peut définir plusieurs paramètres. On commence par un réseau simple une entrée de dimension 12 deux couches cachées de dimension 8 (neurones) chacune, et une chouche de sortie 1 neurne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your first MLP in Keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed=7\n",
    "np.random.seed(7)\n",
    "\n",
    "# Je télécharge ce dataset que l'on eut trouver partut sur es sites cités ci-dessus. \n",
    "path='/Users/boughanem/Pgmes/data/india/pima-indians-diabetes.data.csv'\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = np.loadtxt(path, delimiter=\",\")\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n",
    "\n",
    "# create model / création du modèle \n",
    "# Créer la première couche Cachéé avec 12 neurines et reçoit 8 signaux (entrée) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# La liste des fonctions d'actions proposées par Keras:\n",
    "#https://keras.io/api/layers/activations/\n",
    "    \n",
    "## Le modèle peut être défini comme ceci #######\n",
    "#model = Sequential([\n",
    "#    Dense(12, input_dim=8), Activation('relu'),\n",
    "#    Dense(8),Activation('relu'),\n",
    "#    Dense(1),Activation('sigmoid')\n",
    "#])\n",
    "############\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Compile model : permet de préparer le réseau (le modèle avec ses optmiseurs, ...)\n",
    "# La liste des optimseurs et fonctions Loss: “https://keras.io/losses, https://keras.io/optimizers”\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model : entrainement du modèle, batch : nombre d'exemples à évaluer avant de moifier les poids\n",
    "#epoch : le nombre de fois que l'on repasse les les exemples\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
    "\n",
    "# evaluate the model, Tester le modèle\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Sauvegarder le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_first_DL_model.h5\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Je télécharge le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Je télécharge le modèle.\n",
    "model = keras.models.load_model(\"my_first_DL_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On reprend le premier exemple de réseaux MLP on utilise Tensorflow"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On a exactement le même schéma,la construction du modèle diffère légérement (en fait Tensorflow utilise Keras) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On peut utiliser des bases de tests fournies par Keras (keras.datasets.imdb.load_data) ou tenseflow  (imbd , ...). Dans l'exemple ci dessous on utilise la base de revue/commentaire sur les films. L'idée est de constrire un modèle capable de détecter si une revue est positive u négative.\n",
    "Dans l'exemple ci dessous on utilise la base de revue/commentaire sur les films. \n",
    "L'idée est de construire un modèle capable de détecter si une revue est positive ou négative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un premier exemple ou les données sont déjà pré-traitées l'appel de eras.datasets.imdb.load_data fait le nécessaire. \n",
    "La suite consiste juste construire le modèle, puis l'entrainer  , etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, datasets\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 20000\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=top_words)\n",
    "\n",
    "X_train[0:,]\n",
    "#chaque commentaire sera représenté par un vecteur de 500 mts.\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(250, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Fit the model\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=128)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dans ce second exemple, on part \"from scratch\" on part de textes bruts\n",
    "on fait tous les traitements pour une analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Review b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
      "Label 0\n",
      "Review b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
      "Label 0\n",
      "Label 0 corresponds to neg\n",
      "Label 1 corresponds to pos\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import preprocessing\n",
    "import os\n",
    "\n",
    "dataset_dir='/Users/boughanem/Pgmes/data/sentiment/aclImdb'\n",
    "train_dir=os.path.join(dataset_dir, 'train')\n",
    "test_dir=os.path.join(dataset_dir, 'test')\n",
    "os.listdir(train_dir)\n",
    "\n",
    "# utiliser une tf.data.Dataset un ensemble d'outils  pour gérer des données\n",
    "# En machine learning on a besoin de trois ensmbles : un train, validation et test\n",
    "# ImDB possède deux sets (train et tesst), on construit un ensemble de validation à partir du Train.\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        train_dir, batch_size=batch_size, validation_split=0.2,subset='training', seed=seed)\n",
    "\n",
    "#On fait le même traitement pour toutes les données validation et test\n",
    "\n",
    "# L'ensemble pour la validation\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_dir, batch_size=batch_size, validation_split=0.2, subset=\"validation\", seed=seed)\n",
    "\n",
    "# le test\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    test_dir, batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "# affichage des données de raw_trains_ds, objet de type tf.dataset\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "  for i in range(2):\n",
    "    print(\"Review\", text_batch.numpy()[i])\n",
    "    print(\"Label\", label_batch.numpy()[i])\n",
    "\n",
    "## Les labels 1 ou 0 sont données en fonction du répértoire\n",
    "# il y a deux répértoires pos (devrait avoir 1) et neg (devrait avoir 0)\n",
    "\n",
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des textes: tockenisation, vectorisation , ... à l'aide preprocessing.TextVectorization.\n",
    "Le texte que l'on traite comporte des données balise de tpe \\br non traitées par \n",
    "le module de vectorisation standard, on écrit une fonction custom_standardization(input_data) qui nettoie ces données\n",
    "si votre texte n'a pas de balises utilisez directement la fonction standard (propoée par défaut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b'Silent Night, Deadly Night 5 is the very last of the series, and like part 4, it\\'s unrelated to the first three except by title and the fact that it\\'s a Christmas-themed horror flick.<br /><br />Except to the oblivious, there\\'s some obvious things going on here...Mickey Rooney plays a toymaker named Joe Petto and his creepy son\\'s name is Pino. Ring a bell, anyone? Now, a little boy named Derek heard a knock at the door one evening, and opened it to find a present on the doorstep for him. Even though it said \"don\\'t open till Christmas\", he begins to open it anyway but is stopped by his dad, who scolds him and sends him to bed, and opens the gift himself. Inside is a little red ball that sprouts Santa arms and a head, and proceeds to kill dad. Oops, maybe he should have left well-enough alone. Of course Derek is then traumatized by the incident since he watched it from the stairs, but he doesn\\'t grow up to be some killer Santa, he just stops talking.<br /><br />There\\'s a mysterious stranger lurking around, who seems very interested in the toys that Joe Petto makes. We even see him buying a bunch when Derek\\'s mom takes him to the store to find a gift for him to bring him out of his trauma. And what exactly is this guy doing? Well, we\\'re not sure but he does seem to be taking these toys apart to see what makes them tick. He does keep his landlord from evicting him by promising him to pay him in cash the next day and presents him with a \"Larry the Larvae\" toy for his kid, but of course \"Larry\" is not a good toy and gets out of the box in the car and of course, well, things aren\\'t pretty.<br /><br />Anyway, eventually what\\'s going on with Joe Petto and Pino is of course revealed, and as with the old story, Pino is not a \"real boy\". Pino is probably even more agitated and naughty because he suffers from \"Kenitalia\" (a smooth plastic crotch) so that could account for his evil ways. And the identity of the lurking stranger is revealed too, and there\\'s even kind of a happy ending of sorts. Whee.<br /><br />A step up from part 4, but not much of one. Again, Brian Yuzna is involved, and Screaming Mad George, so some decent special effects, but not enough to make this great. A few leftovers from part 4 are hanging around too, like Clint Howard and Neith Hunter, but that doesn\\'t really make any difference. Anyway, I now have seeing the whole series out of my system. Now if I could get some of it out of my brain. 4 out of 5.', shape=(), dtype=string)\n",
      "Label neg\n",
      "Vectorized review (<tf.Tensor: shape=(1, 500), dtype=int64, numpy=\n",
      "array([[ 1287,   313,  2380,   313,   661,     7,     2,    52,   229,\n",
      "            5,     2,   200,     3,    38,   170,   669,    29,  5492,\n",
      "            6,     2,    83,   297,   549,    32,   410,     3,     2,\n",
      "          186,    12,    29,     4,     1,   191,   510,   549,     6,\n",
      "            2,  8229,   212,    46,   576,   175,   168,    20,     1,\n",
      "         5361,   290,     4,     1,   761,   969,     1,     3,    24,\n",
      "          935,  2271,   393,     7, 14250,  1675,     4,  3747,   250,\n",
      "          148,     4,   112,   436,   761,  3529,   548,     4,  3633,\n",
      "           31,     2,  1331,    28,  2096,     3,  2912,     9,     6,\n",
      "          163,     4,  1006,    20,     2, 15996,    15,    85,    53,\n",
      "          147,     9,   292,    89,   959,  2314,   984,    27,   762,\n",
      "            6,   959,     9,   564,    18,     7,  2140,    32,    24,\n",
      "         1254,    36,     1,    85,     3,  3298,    85,     6,  1410,\n",
      "            3,  1936,     2,  3408,   301,   965,     7,     4,   112,\n",
      "          740,  1977,    12,     1,  2014,  2772,     3,     4,   428,\n",
      "            3,  5177,     6,   512,  1254, 13653,   278,    27,   139,\n",
      "           25,   308,     1,   579,     5,   259,  3529,     7,    92,\n",
      "         8981,    32,     2,  3842,   230,    27,   289,     9,    35,\n",
      "            2,  5712,    18,    27,   144,  2166,    56,     6,    26,\n",
      "           46,   466,  2014,    27,    40,  2745,   657,   212,     4,\n",
      "         1376,  3002,  7080,   183,    36,   180,    52,   920,     8,\n",
      "            2,  4028,    12,   969,     1,   158,    71,    53,    67,\n",
      "           85,  2754,     4,   734,    51, 11882,  1611,   294,    85,\n",
      "            6,     2,  1164,     6,   163,     4,  3408,    15,    85,\n",
      "            6,   717,    85,    44,     5,    24,  7158,     3,    48,\n",
      "          604,     7,    11,   225,   384,    73,    65,    21,   242,\n",
      "           18,    27,   120,   295,     6,    26,   667,   129,  4028,\n",
      "          948,     6,    67,    48,   158,    93, 12474,    27,   120,\n",
      "          372,    24, 13176,    35,     1,    85,    32,  2342,    85,\n",
      "            6,  1008,    85,     8,  2349,     2,   357,   257,     3,\n",
      "         2391,    85,    16,     4,  2812,     2,     1,  2904,    15,\n",
      "           24,   554,    18,     5,   259,  2812,     7,    21,     4,\n",
      "           49,  2904,     3,   201,    44,     5,     2,  1039,     8,\n",
      "            2,   521,     3,     5,   259,    73,   175,   691,   179,\n",
      "          564,   834,   692,   168,    20,    16,   969,     1,     3,\n",
      "        14250,     7,     5,   259,  1960,     3,    14,    16,     2,\n",
      "          161,    63, 14250,     7,    21,     4,   145,   436, 14250,\n",
      "            7,   235,    53,    50,     1,     3,  5262,    84,    27,\n",
      "         2320,    35,     1,     4,  3621,  3076, 10793,    37,    12,\n",
      "           98,  2620,    15,    24,   437,   760,     3,     2,  2158,\n",
      "            5,     2,  7080,  3002,     7,  1960,    99,     3,   212,\n",
      "           53,   236,     5,     4,   654,   270,     5,  2656,     1,\n",
      "            4,  1520,    56,    35,   170,   669,    18,    21,    72,\n",
      "            5,    28,   169,  1597, 11554,     7,   565,     3,  1835,\n",
      "         1084,   722,    37,    46,   529,   306,   300,    18,    21,\n",
      "          187,     6,    96,    11,    86,     4,   166,     1,    35,\n",
      "          170,   669,    23,  2356,   183,    99,    38,  4172,  2274,\n",
      "            3,     1,  2291,    18,    12,   144,    62,    96,    97,\n",
      "         1390,   564,    10,   148,    25,   311,     2,   211,   200,\n",
      "           44,     5,    54,  1432,   148,    45,    10,    98,    75,\n",
      "           46,     5,     9,    44,     5,    54,  1344,   669,    44,\n",
      "            5,   661,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import string\n",
    "import re\n",
    "\n",
    "## Préparation des textes: tockenisation, vectorisation , ... \n",
    "# à laide preprocessing.TextVectorization \n",
    "# Reager les données puis extraie les mots \n",
    "\n",
    "#cette fonction permet de nettoyer le texte (supprimer les balisesbr, split)\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )\n",
    "\n",
    "# On vectorise le texte. On utilise la classe TestVectorization qui fait plusieurs opérations.\n",
    "# lémmatise, split et map chaque mots en un id (einteger) grace à output_mode=int, \n",
    "# on peut ussi limter le nombre de mots àprendre en compte dans le dictonnare 20000;\n",
    "# la taille de l'embedding et la taille de la séquence de text (le vecteur d'entrées, je prends 500 mots)\n",
    "\n",
    "# Model constants.\n",
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization, # fonction créé ci-dessus, voir la classe pour les détails\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\", # on donne aussi une taille de la séquence, ici on construit un vecteur\n",
    "                        # le mode binary permet un sac de mot\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "\n",
    "# on appelle la fonction adapt en donnant le texte pour créer le vocabulaire \n",
    "# En fait map prend l'entrée qui est de la forme x, y (text, label), et garde uniquement x (le text)\n",
    "\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "#la fonction adpat est importante, permet de construire réer un index des mots vers des entiers\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "# Sélectionner quelques données \n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", raw_train_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation de toutes les données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définiton du modèle:\n",
    "La première couche Embedding prend un vecteur d'entiers (de mots), chaque entie est représentés par un vecteur embedding de 128 éléments. On passe ensuite dans un Dropout, puis une couche dense de 100 neuones, puis une un Dropot puis une sortie un neurone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tester avec un seul modèle à la fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On construit le modèle\n",
    "#MLP\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features + 1, embedding_dim),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(100, activation=\"relu\"),\n",
    "  layers.Dropout(0.3), \n",
    "  layers.Dense(1, activation=\"softmax\")])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# enrinement\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=3)\n",
    "\n",
    "#evaluation\n",
    "model.evaluate(test_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Une autre façon de définir le modèle est de le voir comme f(g(w(..(x))))\n",
    "x étant l'entrée et les f,g, w dont des dess transformations (couches du réseau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 37s 58ms/step - loss: 7.6992 - accuracy: 0.4951 - val_loss: 7.5087 - val_accuracy: 0.5076\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 7.7024 - accuracy: 0.4949 - val_loss: 7.5087 - val_accuracy: 0.5076\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 7.7127 - accuracy: 0.4942 - val_loss: 7.5087 - val_accuracy: 0.5076\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 7.6246 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.624587535858154, 0.5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Une autre façon de définir le modèle est de le voir comme f(g(w(..(x))))\n",
    "# x étant l'entrée et les f,g, w les transformations\n",
    "\n",
    "# l'entrée est un vecteurs d'entiers\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# chaque entrée est ensuite mappée en un vecteur embeddings : max_featutes de embeddings\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(100, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "# on projette sur ladernière couche 1 neurone \n",
    "predictions = layers.Dense(1, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Training\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=3)\n",
    "\n",
    "\n",
    "#evaluation\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire une prédiction à partir d'un texte externe,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was great!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dans nore modèle la vectorisation des textes, s'est faite à l'extérieur du modèle.\n",
    "# ceci permet un traitement rapide.\n",
    "\n",
    "examples = [\n",
    "  \"The movie was great!\",\n",
    "  \"The movie was okay.\",\n",
    "  \"The movie was terrible...\"\n",
    "]\n",
    "\n",
    "print(examples[0])\n",
    "examples_vec = vectorize_layer(examples)\n",
    "model.predict(examples_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La prédiction \n",
    "Le mieux est de construire un modèle (exporter le modèle appris vers un modèle nouveau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.7310586],\n",
       "        [0.7310586],\n",
       "        [0.7310586],\n",
       "        ...,\n",
       "        [0.7310586],\n",
       "        [0.7310586],\n",
       "        [0.7310586]],\n",
       "\n",
       "       [[0.7310586],\n",
       "        [0.7310586],\n",
       "        [0.7310586],\n",
       "        ...,\n",
       "        [0.7310586],\n",
       "        [0.7310586],\n",
       "        [0.7310586]],\n",
       "\n",
       "       [[0.7310586],\n",
       "        [0.7310586],\n",
       "        [0.7310586],\n",
       "        ...,\n",
       "        [0.7310586],\n",
       "        [0.7310586],\n",
       "        [0.7310586]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\n",
    "  \"The movie was great!\",\n",
    "  \"The movie was okay.\",\n",
    "  \"The movie was terrible...\"\n",
    "]\n",
    "\n",
    "\n",
    "export_model.predict(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On peut définir un modèle avec la vectorisation comme partie du modèle.\n",
    "Peut être couteux à éviter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut définir un modèle avec la vectorisation comme faisant partie du modèle\n",
    "# Le traitement sera plus long\n",
    "\n",
    "text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "x = vectorize_layer(text_input)\n",
    "x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
    "\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(100, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "# on projette sur la dernière couche 1 neurone \n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model_vec = tf.keras.Model(text_input, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model_vec.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Training\n",
    "model_vec.fit(raw_train_ds, validation_data=raw_val_ds, epochs=3)\n",
    "\n",
    "\n",
    "#evaluation\n",
    "model_vec.evaluate(raw_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modèle à Convolution (CNN)\n",
    "\n",
    "Ce type de modèle utilisé pour les images (le premier exemple donneu exemple de recherche d'image à partir d'u dataset cifar10).\n",
    "Le second exemple exploite un CNN sur notre texte.\n",
    "### 1 Exemple classification d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "#model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Exemple : un modèle CNN sur notre base de textes\n",
    "on utilise les versions vectorisées dans nos textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - 47s 74ms/step - loss: 0.6371 - accuracy: 0.5727 - val_loss: 0.3262 - val_accuracy: 0.8620\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 50s 80ms/step - loss: 0.2908 - accuracy: 0.8843 - val_loss: 0.3114 - val_accuracy: 0.8782\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.1371 - accuracy: 0.9504 - val_loss: 0.4469 - val_accuracy: 0.8804\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.0659 - accuracy: 0.9768 - val_loss: 0.5537 - val_accuracy: 0.8654\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 45s 73ms/step - loss: 0.0331 - accuracy: 0.9896 - val_loss: 0.5636 - val_accuracy: 0.8808\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 45s 73ms/step - loss: 0.0307 - accuracy: 0.9884 - val_loss: 0.6306 - val_accuracy: 0.8754\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.0242 - accuracy: 0.9912 - val_loss: 0.6398 - val_accuracy: 0.8698\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.0186 - accuracy: 0.9934 - val_loss: 0.6722 - val_accuracy: 0.8762\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.0200 - accuracy: 0.9935 - val_loss: 0.7660 - val_accuracy: 0.8770\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.0146 - accuracy: 0.9951 - val_loss: 0.7508 - val_accuracy: 0.8734\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 45s 72ms/step - loss: 0.0173 - accuracy: 0.9948 - val_loss: 0.6988 - val_accuracy: 0.8796\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.0137 - accuracy: 0.9955 - val_loss: 0.8134 - val_accuracy: 0.8764\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 45s 72ms/step - loss: 0.0124 - accuracy: 0.9963 - val_loss: 0.8857 - val_accuracy: 0.8676\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 1506s 2s/step - loss: 0.0092 - accuracy: 0.9971 - val_loss: 1.0215 - val_accuracy: 0.8780\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 52s 82ms/step - loss: 0.0098 - accuracy: 0.9960 - val_loss: 1.0097 - val_accuracy: 0.8656\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 46s 74ms/step - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.9958 - val_accuracy: 0.8734\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 45s 72ms/step - loss: 0.0090 - accuracy: 0.9977 - val_loss: 0.9367 - val_accuracy: 0.8768\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 46s 74ms/step - loss: 0.0092 - accuracy: 0.9967 - val_loss: 0.7628 - val_accuracy: 0.8810\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 46s 74ms/step - loss: 0.0082 - accuracy: 0.9971 - val_loss: 0.9469 - val_accuracy: 0.8760\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.0080 - accuracy: 0.9977 - val_loss: 1.3622 - val_accuracy: 0.8664\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.3953 - accuracy: 0.8496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3953273296356201, 0.8496400117874146]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# A integer input for vocab indices.\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model_cnn = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model_cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "model_cnn.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "\n",
    "# evaluate\n",
    "model_cnn.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De même pour prédire un texte externe (à la base de test)\n",
    "On crée un oveau modèle on ajoute juste la vectorisation des entrées.\n",
    "le modèle utilise les poids appris précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = model_cnn(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was great!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6432649 ],\n",
       "       [0.3580997 ],\n",
       "       [0.13735926]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "examples = [\n",
    "  \"The movie was great!\",\n",
    "  \"The movie was okay.\",\n",
    "  \"The movie was terrible...\"\n",
    "]\n",
    "\n",
    "print(examples[0])\n",
    "\n",
    "\n",
    "end_to_end_model.predict(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
