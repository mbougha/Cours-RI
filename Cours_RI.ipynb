{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cours Introduction à la Recherche d'Information à l'N7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexation, il existe différentes librairies sous Python pour analyser le texte. \n",
    "## L'analyse peut consister à extraire les mots simples, jusqu'à l'analyse un peu plus \"poussée\". Vecteurs de mots pondérés, jusqu'aux embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De manière générale le  \"text processing\" passe par les étapes suivantes\n",
    "    - tokenizer—scanner, lexer, lexical analyzer\n",
    "    - vocabulary—lexicon\n",
    "    - token, term, word, or n-gram—token, symbol, or terminal symbol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Des exemples de Librairies : NLTK, Sklearn, Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize #TweetTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "#NLTK\n",
    "#- tokenizer—scanner, lexical analyzer\n",
    "def extraire_bag_of_words(texte, type_norma):  \n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Method-1 : Individual words as separate elements of the list   \n",
    "    #text split extraire tous les mots, séparateurs, ...\n",
    "    texte_split=texte.split()\n",
    "    \n",
    "    # seuls les mots (tokens) sont extraits) \n",
    "    texte = word_tokenize(texte.lower())\n",
    "\n",
    "    # suppression des stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    #Liste des mots sans les mots vides\n",
    "    new_tokens = [w for w in texte if not w in stop_words]\n",
    "    # Liste des mots normalisés (selon Porter)\n",
    "    if type_norma=='porter':      \n",
    "        new_tokens = [stemmer.stem(w) for w in texte if not w in stop_words]\n",
    "        #print(\"Normalisation selon Porter\")\n",
    "        #print(new_tokens)\n",
    "\n",
    "    # Liste des mots normalisés (selon Lemmatiseur)\n",
    "    if type_norma=='lemmatizer':      \n",
    "        new_tokens = [lemmatizer.lemmatize(w) for w in texte if not w in stop_words]\n",
    "        #print(\"Normalisation selon lemmatiseur\")\n",
    "        #print(new_tokens)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def count_words(tokens):\n",
    "    # librairie pour le comptage des mots\n",
    "    from collections import Counter\n",
    "    bag_of_words = Counter(tokens)\n",
    "    return bag_of_words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = 'data/sample.txt'\n",
    "with open(text_file) as f:\n",
    "        text=f.read()\n",
    "\n",
    "##   Extraire les mots simple et lémamtiser avec Porter ou un lemattizar plus sophistiqué (par exemple WordNet)\n",
    "\n",
    "tokens=extraire_bag_of_words(text,'porter')\n",
    "#tokens=extraire_bag_of_words(text,'lemmatizer')\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "#récuperer la liste des n grammes (récupérer les mots adjenents 2 à deux.)\n",
    "list(ngrams(tokens,2))\n",
    "\n",
    "#Le texte peut comporter plusieurs phrases on peut \"Split\" les phrases (passages) du texte.\n",
    "print(sent_tokenize(text))\n",
    "\n",
    "# compter les tokens (construire le bag of words (Mot,count))\n",
    "#bag_words=count_words(tokens)\n",
    "#print(bag_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uilisation de WordNet our une représentation \"conceptuelle\"\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "word_ = wordnet.synsets(\"spectacular\")\n",
    "print (word_[2].definition())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- SKLEARN : Vectorisation des textes avec sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP avec scikit-learn \n",
    "# Libraire pour la vectorization des textes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#operation en deux étapes fit (pour le vocabulaire) puis tranform (pour construire les vecteurs)  \n",
    "def vectorize_2_steps(text):\n",
    "    cv = CountVectorizer() \n",
    "    #vectorization    \n",
    "    vectors = cv.transform(text)      \n",
    "    return vectors, cv.get_feature_names()\n",
    "\n",
    "#operation fit et transform avec une seule fonction \n",
    "def vectorize_1_step(text):\n",
    "    cv = CountVectorizer() \n",
    "    vectors=cv.fit_transform(text)    \n",
    "    return vectors, cv.get_feature_names()\n",
    "\n",
    "def simple_vector(text):\n",
    "    cv = CountVectorizer(stop_words='english',token_pattern=r'\\w+')\n",
    "    vectors = cv.fit_transform(text).todense()  \n",
    "    return vectors, cv.get_feature_names()\n",
    "\n",
    "def tf_idf_vector(text):\n",
    "    t = TfidfVectorizer(stop_words='english', token_pattern=r'\\w+')\n",
    "    vectors = t.fit_transform(text).todense()\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sur la vectorisation avec SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = 'data/sample.txt'\n",
    "\n",
    "with open(text_file) as f:    \n",
    "    text=f.readlines()\n",
    "\n",
    "# Construire les vecteurs \n",
    "#Les fonction CountVectorizer, simple façon de \"tokeniser\" un texte, \n",
    "\n",
    "#vectors,features = vectorize_1_step(text)\n",
    "\n",
    "# vecteur dense\n",
    "#vectors,features = simple_vector(text)\n",
    "#print(vectors.shape)\n",
    "#print(features)\n",
    "#print(vectors)\n",
    "#print(vectors.toarray())  # affichage mieux que print(vectors) si le vectru n'est pas mis en \"todense()\" dans la fonction\n",
    "\n",
    "vectors= tf_idf_vector(text)\n",
    "print(vectors.shape)\n",
    "print(vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Librairie GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La première partie donne un exemple de \"tokenisation\" assez simple , on crée un dictionnaire, puis on transfome les mots en id (tokek_2id). On crée un simple dictionnaire à partir de d'un texte, d'un fichier ou à partir d'un répétoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import os\n",
    "# Choisir une option lecture du Texte et la procédure de construction des tockens\n",
    "            # un simple texte\n",
    "Text =[\"\"\"Global warming is a long-term rise in the average temperature of the Earth s climate system, \n",
    "       an aspect of climate change shown by temperature measurements and by multiple effects of the warming.\"\"\"]\n",
    "\n",
    "            ## version simple lectrure ligne par phrase puis split()\n",
    "#tokens = [[token for token in sentence.split()] for sentence in text]\n",
    "            ## utilisation du pre_pocessing de gensim\n",
    "#tokens= [simple_preprocess(remove_stopwords(sentence)) for sentence in Text]\n",
    "            #avec suprenssion de mots vides\n",
    "#tokens = [simple_preprocess(remove_stopwords(sentence)) for sentence in Text]\n",
    "\n",
    "            ##Texte dans un Un fichier\n",
    "#tokens=[simple_preprocess(sentence) for sentence in open('data/dataN7/text_file1.txt', encoding='utf-8')]\n",
    "\n",
    "            ## Plusieurs fichiers dans un répértoire\n",
    "dir_path= 'data/dataN7'\n",
    "for file_name in os.listdir(dir_path):\n",
    "    if \".txt\" in file_name:  \n",
    "        tokens = [simple_preprocess(remove_stopwords(sentence)) for sentence in open(os.path.join(dir_path, file_name), encoding='utf-8')]\n",
    "\n",
    "    ##Construire le dictionnaire\n",
    "gensim_dictionary = corpora.Dictionary(tokens)\n",
    "    ##Afficher les mots\n",
    "gensim_dictionary.token2id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On crée un sac de mots (Id, frequence). On utilise  la méthode  doc2bow, de gensim_dictionary.Chaque mot est traité séparemment, si il existe dans le dictionnaire sa fréquence est incrémentée sinon il est crée avec ne fréquence de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "import os\n",
    "\n",
    "    ## on lit les documents à partir d'un fichier (pas besoin de lire plusieurs fois le fichier, je le fais pour que chque cellule puisse être exécutée séparement.)\n",
    "dir_path= 'data/dataN7'\n",
    "for file_name in os.listdir(dir_path):\n",
    "    if \".txt\" in file_name:  \n",
    "        print(file_name)\n",
    "        tokens = [simple_preprocess(remove_stopwords(sentence)) for sentence in open(os.path.join(dir_path, file_name), encoding='utf-8')]\n",
    "\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True) for token in tokens]\n",
    "\n",
    "    ##Afficher les ID ainsi que leeur fréquence)\n",
    "print(gensim_corpus)\n",
    "\n",
    "    ## on peut aussi aficher les mots ainsi que leur fréqueence\n",
    "word_frequencies = [[(gensim_dictionary[id], frequence) for id, frequence in couple] for couple in gensim_corpus]\n",
    "\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On passe à la vectorisation (on crée un bag of words pondéré par tf.idf (a la SMART (modèle vectriel))). On utilise la librairie models de gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "    ## Je récupre le coprus que j'ai construit précédemment\n",
    "    ## J'appelle la méthode TfidfModel de mdels\n",
    "\n",
    "tfidf = models.TfidfModel(gensim_corpus, smartirs='ltc')\n",
    "\n",
    "for doc in tfidf[gensim_corpus]:\n",
    "    print(doc)\n",
    "    for id, frequency in doc:\n",
    "        print(gensim_dictionary[id], np.around(frequency, decimals=2))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LISI et LDA\n",
    "## LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-b2d2c8b5ea17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLsiModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgensim_corpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "lsi = models.LsiModel(corpus=tfidf[gensim_corpus], id2word=my_dict, num_topics=4) \n",
    "\n",
    "#corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "#lsi.print_topics(-1)\n",
    "lsi.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creation des ngrams\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "text_file = 'data/sample.txt'\n",
    "all_docs=[]\n",
    "with open(text_file) as f:    \n",
    "    text=f.readlines()\n",
    "for doc in text:\n",
    "    all_docs.append(doc)\n",
    "    \n",
    "#dct = corpora.Dictionary(dataset)\n",
    "#corpus = [dct.doc2bow(line) for line in dataset]\n",
    "\n",
    "# Build the bigram models\n",
    "bigram = gensim.models.phrases.Phrases(all_docs, min_count=2, threshold=10)\n",
    "\n",
    "# Construct bigram\n",
    "print(bigram[all_docs[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "#LSI\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "\n",
    "#my_dict, my_corpus=get_dict_corpus()\n",
    "    \n",
    "# initialize an LSI transformation\n",
    "my_dict = corpora.Dictionary.load('data/my_dict.dict')\n",
    "my_corpus = corpora.MmCorpus('data/bow_corpus.mm')\n",
    "\n",
    "#reécupérer ue représentation d'un document\n",
    "\n",
    "#print('LDA')\n",
    "lda_model = models.LdaMulticore(my_corpus,id2word=my_dict,random_state=100,num_topics=5)\n",
    "\n",
    "#lda_model.print_topics(-1)\n",
    "lda_model.show_topics()\n",
    "\n",
    "\n",
    "# lire les fichiers\n",
    "\n",
    "lda_model[my_corpus]\n",
    "\n",
    "#for c in lda_model[my_corpus[0:3]]:\n",
    "#    print(\"Document Topics      : \", c[0])      # [(Topics, Perc Contrib)]\n",
    "#    print(\"Word id, Topics      : \", c[1][:])  # [(Word id, [Topics])]\n",
    "#    print(\"Phi Values (word id) : \", c[2][:2])  # [(Word id, [(Topic, Phi Value)])]\n",
    "#    print(\"Word, Topics         : \", [(my_dict[wd], topic) for wd, topic in c[1][:2]])   # [(Word, [Topics])]\n",
    "#    print(\"Phi Values (word)    : \", [(my_dict[wd], topic) for wd, topic in c[2][:2]])  # [(Word, [(Topic, Phi Value)])]\n",
    "    #print(\"------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=gensim_preprocess_data()\n",
    "data[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "['this', 'is', 'the', 'second', 'sentence'],\n",
    "['yet', 'another', 'sentence'],\n",
    "['one', 'more', 'sentence'],\n",
    "['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "data[1:10]\n",
    "\n",
    "model = Word2Vec(data, min_count = 1, workers=cpu_count())\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import os\n",
    "\n",
    "\n",
    "dir_path= 'data/dataN7'\n",
    "\n",
    "for file_name in os.listdir(dir_path):\n",
    "    if \".txt\" in file_name:  \n",
    "        print(file_name)\n",
    "        tokens = [simple_preprocess(sentence) for sentence in open(os.path.join(dir_path, file_name), encoding='utf-8')]\n",
    "\n",
    "#print(tokens)\n",
    "        \n",
    "\n",
    "model = Word2Vec(tokens, min_count = 1, workers=cpu_count())\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "#print(model['sentence'])\n",
    "\n",
    "# access vector for one word\n",
    "print(model['earth'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save('data/dataN7/w2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model en ascii \n",
    "model.wv.save_word2vec_format('data/dataN7/w2vec_model_ascii', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "model = Word2Vec.load('data/dataN7/w2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser des vecteurs (W2VEC) Préentrainés. Word2Vec, Glove et FastText ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download the models\n",
    "print(\"ça pourrait prendre un peu de temps si vous l'avez pas localement dans la librarire de gensim\")\n",
    "#fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "#glove_model300 = api.load('glove-wiki-gigaword-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import softcossim\n",
    "from gensim import corpora\n",
    "\n",
    "sent_1 = 'Sachin is a cricket player and a opening batsman'.split()\n",
    "sent_2 = 'Dhoni is a cricket player too He is a batsman and keeper'.split()\n",
    "sent_3 = 'Anand is a chess player'.split()\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# Prepare a dictionary and a corpus.\n",
    "documents = [sent_1, sent_2, sent_3]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Convert the sentences into bag-of-words vectors.\n",
    "sent_1 = dictionary.doc2bow(sent_1)\n",
    "sent_2 = dictionary.doc2bow(sent_2)\n",
    "sent_3 = dictionary.doc2bow(sent_3)\n",
    "\n",
    "# Compute soft cosine similarity\n",
    "print(softcossim(sent_1, sent_2, similarity_matrix))\n",
    "\n",
    "print(softcossim(sent_1, sent_3, similarity_matrix))\n",
    "\n",
    "print(softcossim(sent_2, sent_3, similarity_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "# Load them back\n",
    "my_dict = corpora.Dictionary.load('data/my_dict.dict')\n",
    "my_corpus = corpora.MmCorpus('data/bow_corpus.mm')\n",
    "#for line in my_corpus:\n",
    "#    print(line)\n",
    "    \n",
    "print(\"Le premier doc de mon copus\\n\",my_corpus[0])\n",
    "# Create the TF-IDF model\n",
    "tfidf = models.TfidfModel(my_corpus, smartirs='ntc')\n",
    "\n",
    "print(\"Le premier doc de mon TFIDF\\n\",tfidf[my_corpus[0]])\n",
    "print(tfidf[my_corpus[0]])  #apply model to the first corpus document\n",
    "\n",
    "# Show the Word Weights in Corpus\n",
    "print(\"TOUTE LA MATRICE TD-IDF\\n\");\n",
    "for doc in tfidf[my_corpus]:\n",
    "    print([[my_dict[id], np.around(freq, decimals=2)] for id, freq in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 0\n",
    "size = 50\n",
    "window = 3\n",
    "corpus=[]\n",
    "\n",
    "#construire son propre w2Vec\n",
    "for sentence in texte:\n",
    "    corpus.append(sentence.split())\n",
    "    model = Word2Vec(corpus, min_count=min_count, size=size, window=window)\n",
    "\n",
    "#pprint(model.most_similar('the'))\n",
    "# appeler un W2Vec \n",
    "#print(\"début\")\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#pprint(\"fin\")\n",
    "#pprint(model.most_similar('hello'))\n",
    "#pprint(\"fin2\")                                                     \n",
    "#tokens_s=[]\n",
    "#for token in texte:\n",
    "    # tokens_s.append(stemmer.stem(token))\n",
    "    # tokens_s.append(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an': 0,\n",
       " 'and': 1,\n",
       " 'aspect': 2,\n",
       " 'average': 3,\n",
       " 'by': 4,\n",
       " 'change': 5,\n",
       " 'climate': 6,\n",
       " 'earth': 7,\n",
       " 'effects': 8,\n",
       " 'global': 9,\n",
       " 'in': 10,\n",
       " 'is': 11,\n",
       " 'long': 12,\n",
       " 'measurements': 13,\n",
       " 'multiple': 14,\n",
       " 'of': 15,\n",
       " 'rise': 16,\n",
       " 'shown': 17,\n",
       " 'system': 18,\n",
       " 'temperature': 19,\n",
       " 'term': 20,\n",
       " 'the': 21,\n",
       " 'warming': 22,\n",
       " 'air': 23,\n",
       " 'also': 24,\n",
       " 'caused': 25,\n",
       " 'commonly': 26,\n",
       " 'continuing': 27,\n",
       " 'earlier': 28,\n",
       " 'economy': 29,\n",
       " 'emissions': 30,\n",
       " 'episodes': 31,\n",
       " 'experienced': 32,\n",
       " 'gasses': 33,\n",
       " 'geological': 34,\n",
       " 'greenhouse': 35,\n",
       " 'increase': 36,\n",
       " 'industrial': 37,\n",
       " 'mainly': 38,\n",
       " 'modern': 39,\n",
       " 'observed': 40,\n",
       " 'ocean': 41,\n",
       " 'periods': 42,\n",
       " 'refers': 43,\n",
       " 'since': 44,\n",
       " 'temperatures': 45,\n",
       " 'though': 46,\n",
       " 'to': 47}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "# Create gensim dictionary form a single tet file\n",
    "dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('data/dataN7/text_file1.txt', encoding='utf-8'))\n",
    "\n",
    "dictionary.token2id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
