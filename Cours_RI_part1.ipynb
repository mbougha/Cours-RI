{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cours Introduction à la Recherche d'Information à l'N7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexation, il existe différentes librairies sous Python pour analyser le texte. \n",
    "## L'analyse peut consister à extraire les mots simples, jusqu'à l'analyse un peu plus \"poussée\". Vecteurs de mots pondérés, jusqu'aux embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De manière générale le  \"text processing\" passe par les étapes suivantes\n",
    "    - tokenizer—scanner, lexer, lexical analyzer\n",
    "    - vocabulary—lexicon\n",
    "    - token, term, word, or n-gram—token, symbol, or terminal symbol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Des exemples de Librairies : NLTK, Sklearn, Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize #TweetTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "#NLTK\n",
    "#- tokenizer—scanner, lexical analyzer\n",
    "def extraire_bag_of_words(texte, type_norma):  \n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Method-1 : Individual words as separate elements of the list   \n",
    "    #text split extraire tous les mots, séparateurs, ...\n",
    "    texte_split=texte.split()\n",
    "    \n",
    "    # seuls les mots (tokens) sont extraits) \n",
    "    texte = word_tokenize(texte.lower())\n",
    "\n",
    "    # suppression des stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    #Liste des mots sans les mots vides\n",
    "    new_tokens = [w for w in texte if not w in stop_words]\n",
    "    # Liste des mots normalisés (selon Porter)\n",
    "    if type_norma=='porter':      \n",
    "        new_tokens = [stemmer.stem(w) for w in texte if not w in stop_words]\n",
    "        #print(\"Normalisation selon Porter\")\n",
    "        #print(new_tokens)\n",
    "\n",
    "    # Liste des mots normalisés (selon Lemmatiseur)\n",
    "    if type_norma=='lemmatizer':      \n",
    "        new_tokens = [lemmatizer.lemmatize(w) for w in texte if not w in stop_words]\n",
    "        #print(\"Normalisation selon lemmatiseur\")\n",
    "        #print(new_tokens)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def count_words(tokens):\n",
    "    # librairie pour le comptage des mots\n",
    "    from collections import Counter\n",
    "    bag_of_words = Counter(tokens)\n",
    "    return bag_of_words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = 'data/sample.txt'\n",
    "with open(text_file) as f:\n",
    "        text=f.read()\n",
    "\n",
    "##   Extraire les mots simple et lémamtiser avec Porter ou un lemattizar plus sophistiqué (par exemple WordNet)\n",
    "\n",
    "tokens=extraire_bag_of_words(text,'porter')\n",
    "#tokens=extraire_bag_of_words(text,'lemmatizer')\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "#récuperer la liste des n grammes (récupérer les mots adjenents 2 à deux.)\n",
    "list(ngrams(tokens,2))\n",
    "\n",
    "#Le texte peut comporter plusieurs phrases on peut \"Split\" les phrases (passages) du texte.\n",
    "print(sent_tokenize(text))\n",
    "\n",
    "# compter les tokens (construire le bag of words (Mot,count))\n",
    "#bag_words=count_words(tokens)\n",
    "#print(bag_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uilisation de WordNet our une représentation \"conceptuelle\"\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "word_ = wordnet.synsets(\"spectacular\")\n",
    "print (word_[2].definition())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- SKLEARN : Vectorisation des textes avec sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP avec scikit-learn \n",
    "# Libraire pour la vectorization des textes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#operation en deux étapes fit (pour le vocabulaire) puis tranform (pour construire les vecteurs)  \n",
    "def vectorize_2_steps(text):\n",
    "    cv = CountVectorizer() \n",
    "    #vectorization    \n",
    "    vectors = cv.transform(text)      \n",
    "    return vectors, cv.get_feature_names()\n",
    "\n",
    "#operation fit et transform avec une seule fonction \n",
    "def vectorize_1_step(text):\n",
    "    cv = CountVectorizer() \n",
    "    vectors=cv.fit_transform(text)    \n",
    "    return vectors, cv.get_feature_names()\n",
    "\n",
    "def simple_vector(text):\n",
    "    cv = CountVectorizer(stop_words='english',token_pattern=r'\\w+')\n",
    "    vectors = cv.fit_transform(text).todense()  \n",
    "    return vectors, cv.get_feature_names()\n",
    "\n",
    "def tf_idf_vector(text):\n",
    "    t = TfidfVectorizer(stop_words='english', token_pattern=r'\\w+')\n",
    "    vectors = t.fit_transform(text).todense()\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sur la vectorisation avec SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = 'data/sample.txt'\n",
    "\n",
    "with open(text_file) as f:    \n",
    "    text=f.readlines()\n",
    "\n",
    "# Construire les vecteurs \n",
    "#Les fonction CountVectorizer, simple façon de \"tokeniser\" un texte, \n",
    "\n",
    "#vectors,features = vectorize_1_step(text)\n",
    "\n",
    "# vecteur dense\n",
    "#vectors,features = simple_vector(text)\n",
    "#print(vectors.shape)\n",
    "#print(features)\n",
    "#print(vectors)\n",
    "#print(vectors.toarray())  # affichage mieux que print(vectors) si le vectru n'est pas mis en \"todense()\" dans la fonction\n",
    "\n",
    "vectors= tf_idf_vector(text)\n",
    "print(vectors.shape)\n",
    "print(vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Librairie GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La première partie donne un exemple de \"tokenisation\" assez simple , on crée un dictionnaire, puis on transfome les mots en id (token2id). On crée un simple dictionnaire à partir d'un texte, d'un fichier ou un répétoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "import os\n",
    "# Choisir une option lecture du Texte et la procédure de construction des tockens\n",
    "            # un simple texte\n",
    "#Text =[\"\"\"Global warming is a long-term rise in the average temperature of the Earth s climate system, \n",
    "#       an aspect of climate change shown by temperature measurements and by multiple effects of the warming.\"\"\"]\n",
    "\n",
    "            ## version simple lectrure ligne par phrase puis split()\n",
    "#tokens = [[token for token in sentence.split()] for sentence in text]\n",
    "            ## utilisation du pre_pocessing de gensim\n",
    "#tokens= [simple_preprocess(remove_stopwords(sentence)) for sentence in Text]\n",
    "            #avec suprenssion de mots vides\n",
    "#tokens = [simple_preprocess(remove_stopwords(sentence)) for sentence in Text]\n",
    "\n",
    "            ##Texte dans un Un fichier\n",
    "#tokens=[simple_preprocess(sentence) for sentence in open('data/dataN7/text_file1.txt', encoding='utf-8')]\n",
    "\n",
    "## Plusieurs fichiers dans un répértoire\n",
    "dir_path= 'data/dataN7'\n",
    "for file_name in os.listdir(dir_path):\n",
    "    if \".txt\" in file_name:  \n",
    "        tokens = [simple_preprocess(remove_stopwords(sentence)) for sentence in open(os.path.join(dir_path, file_name), encoding='utf-8')]\n",
    "\n",
    "    ##Construire le dictionnaire\n",
    "gensim_dictionary = corpora.Dictionary(tokens)\n",
    "\n",
    "##Afficher les mots\n",
    "gensim_dictionary.token2id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On crée un sac de mots (Id, frequence). On utilise  la méthode  doc2bow, de gensim_dictionary.Chaque mot est traité séparemment, si il existe dans le dictionnaire sa fréquence est incrémentée sinon il est crée avec ne fréquence de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "import os\n",
    "\n",
    "## on lit les documents à partir d'un fichier (pas besoin de lire plusieurs fois le fichier, je le fais pour que chque cellule puisse être exécutée séparement.)\n",
    "dir_path= 'data/dataN7'\n",
    "for file_name in os.listdir(dir_path):\n",
    "    if \".txt\" in file_name:  \n",
    "        print(file_name)\n",
    "        tokens = [simple_preprocess(remove_stopwords(sentence)) for sentence in open(os.path.join(dir_path, file_name), encoding='utf-8')]\n",
    "\n",
    "gensim_dictionary = corpora.Dictionary(tokens)\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True) for token in tokens]\n",
    "\n",
    "##Afficher les ID ainsi que leeur fréquence)\n",
    "print(gensim_corpus)\n",
    "\n",
    "    ## on peut aussi aficher les mots ainsi que leur fréqueence\n",
    "word_frequencies = [[(gensim_dictionary[id], frequence) for id, frequence in couple] for couple in gensim_corpus]\n",
    "\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On passe à la vectorisation (on crée un bag of words pondéré par tf.idf (a la SMART (modèle vectriel))). On utilise la librairie models de gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "    ## Je récupre le coprus que j'ai construit précédemment\n",
    "    ## J'appelle la méthode TfidfModel de mdels\n",
    "\n",
    "tfidf = models.TfidfModel(gensim_corpus, smartirs='ltc')\n",
    "\n",
    "for doc in tfidf[gensim_corpus]:\n",
    "    print(doc)\n",
    "    for id, frequency in doc:\n",
    "        print(gensim_dictionary[id], np.around(frequency, decimals=2))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LISI et LDA\n",
    "## LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.info(\"text8\")  # return dict with info about \"text8\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import LsiModel\n",
    "import os\n",
    "\n",
    "#Etape 1 récuperer les documents\n",
    "## on lit les documents à partir d'un fichier (pas besoin de lire plusieurs fois le fichier, \n",
    "#je le fais pour que chque cellule puisse être exécutée séparement. On récupère les mots lémmatisés)\n",
    "#dir_path= 'data/dataN7'\n",
    "#for file_name in os.listdir(dir_path):\n",
    "#    if \".txt\" in file_name:  \n",
    "#        print(file_name)\n",
    "#        tokens = [simple_preprocess(remove_stopwords(sentence)) for sentence in open(os.path.join(dir_path, file_name), encoding='utf-8')]\n",
    "\n",
    "#Prendcre un GROS fichier exemple proposé par Gensim\n",
    "dataset = api.load(\"text8\")\n",
    "tokens = [d for d in dataset]\n",
    "\n",
    "##Afficher les ID ainsi que leur fréquence)\n",
    "#print(gensim_corpus)\n",
    "\n",
    "\n",
    "# Step 3: Créer les entrées de LSI : Dictionnaire et le corpus \n",
    "        \n",
    "gensim_dictionary = corpora.Dictionary(tokens)\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True) for token in tokens]\n",
    "\n",
    "lsi = models.LsiModel(corpus=gensim_corpus, id2word=gensim_dictionary, num_topics=4) \n",
    "\n",
    "lsi.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On récupère le corpus et le dictionnaire précédents :  gensim_dictionary  et gensim_corpus\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "\n",
    "lda_model = LdaMulticore(corpus=gensim_corpus,\n",
    "                         id2word=gensim_dictionary,\n",
    "                         random_state=100,\n",
    "                         num_topics=7,\n",
    "                         passes=10,\n",
    "                         chunksize=1000,\n",
    "                         batch=False,\n",
    "                         alpha='asymmetric',\n",
    "                         decay=0.5,\n",
    "                         offset=64,\n",
    "                         eta=None,\n",
    "                         eval_every=0,\n",
    "                         iterations=100,\n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "# save the model\n",
    "lda_model.save('lda_model.model')\n",
    "\n",
    "lda_model.print_topics(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in lda_model[gensim_corpus[5:8]]:\n",
    "    print(\"Document Topics      : \", c[0])      # [(Topics, Perc Contrib)]\n",
    "    print(\"Word id, Topics      : \", c[1][:3])  # [(Word id, [Topics])]\n",
    "    print(\"Phi Values (word id) : \", c[2][:2])  # [(Word id, [(Topic, Phi Value)])]\n",
    "    print(\"Word, Topics         : \", [(dct[wd], topic) for wd, topic in c[1][:2]])   # [(Word, [Topics])]\n",
    "    print(\"Phi Values (word)    : \", [(dct[wd], topic) for wd, topic in c[2][:2]])  # [(Word, [(Topic, Phi Value)])]\n",
    "    print(\"------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "#LSI\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "\n",
    "#my_dict, my_corpus=get_dict_corpus()\n",
    "    \n",
    "# initialize an LSI transformation\n",
    "my_dict = corpora.Dictionary.load('data/my_dict.dict')\n",
    "my_corpus = corpora.MmCorpus('data/bow_corpus.mm')\n",
    "\n",
    "#reécupérer ue représentation d'un document\n",
    "\n",
    "#print('LDA')\n",
    "lda_model = models.LdaMulticore(my_corpus,id2word=my_dict,random_state=100,num_topics=5)\n",
    "\n",
    "#lda_model.print_topics(-1)\n",
    "lda_model.show_topics()\n",
    "\n",
    "\n",
    "# lire les fichiers\n",
    "\n",
    "lda_model[my_corpus]\n",
    "\n",
    "#for c in lda_model[my_corpus[0:3]]:\n",
    "#    print(\"Document Topics      : \", c[0])      # [(Topics, Perc Contrib)]\n",
    "#    print(\"Word id, Topics      : \", c[1][:])  # [(Word id, [Topics])]\n",
    "#    print(\"Phi Values (word id) : \", c[2][:2])  # [(Word id, [(Topic, Phi Value)])]\n",
    "#    print(\"Word, Topics         : \", [(my_dict[wd], topic) for wd, topic in c[1][:2]])   # [(Word, [Topics])]\n",
    "#    print(\"Phi Values (word)    : \", [(my_dict[wd], topic) for wd, topic in c[2][:2]])  # [(Word, [(Topic, Phi Value)])]\n",
    "    #print(\"------------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=gensim_preprocess_data()\n",
    "data[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Sous GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser des vecteurs (W2VEC) Préentrainés : Word2Vec, Glove et FastText \n",
    "### Utilisation de Word2Vec (WOrd2Vec est sur mon ordinateur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "# load the google word2vec model\n",
    "filename = '../../data/GoogleNews-vectors-negative300.bin'\n",
    "if os.path.exists(filename) : \n",
    "    #charge le model en mémoire\n",
    "    model1 = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "else:\n",
    "    print('le fichier n existe pas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utiisation de Glove (nécessite une conversion vers le format Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#Utilisation de Glove de Stanford \n",
    "#The first step is to convert the GloVe file format to the word2vec file format. \n",
    "glove_input_file = '../../data/Glove/glove.6B.100d.txt'\n",
    "glove_output_file = '../../data/Glove/glove.6B.100d.txt.word2vec'\n",
    "#convert txt en word2Vec Format\n",
    "glove2word2vec(glove_input_file, glove_output_file)\n",
    "\n",
    "# UNE FOIS CONVERTI, On UTILISE LE MOD7LE comme ci-dessous\n",
    "# load the converted model\n",
    "filename = '../../data/Glove/glove.6B.100d.txt.word2vec'\n",
    "glove_model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim propose une api (ci dessous) qui permet de charger le fichier (à prtir de votre ordniteur si il existe dans gensim-data/ ou , le télécharger à partir d'un site extérieur ça peut prendre du temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim propose une api (ci dessous) qui permet de charger le fichier (de votre ordnitauer si il existe dans gensim-data/ \n",
    "#ou , le téléchargé à partir d'un site extérieur ça peut prendre du temps\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download the models\n",
    "#ça pourrait prendre un peu de temps si vous l'avez pas localement dans la librarire de gensim\")\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "glove_model300 = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On peut jouer avec les modèles chargés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calul de la similarité\n",
    "\n",
    "# avec Word2Vec\n",
    "word2vec_model300.most_similar('trump',topn=5)\n",
    "\n",
    "# avec FastText\n",
    "fasttext_model300.most_similar('trump',topn=5)\n",
    "\n",
    "# Avec Glove\n",
    "glove_model300.most_similar('trump',topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cmme on maniplue des vecteurs , on peut s'amuser à faire ce type de calcul (Vect(king)-Vect(man)+Vect(woman) .... le vecteur résultat est proche du vecteur du mot \"Queen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate: (king - man) + woman = ?\n",
    "result = word2vec_model300.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "#result = glove_model.most_similar(positive=['france', 'china'], negative=['paris'], topn=3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent_1 = 'Sachin is a cricket player and a opening batsman'.split()\n",
    "sent_2 = 'Dhoni is a cricket player too He is a batsman and keeper'.split()\n",
    "sent_3 = 'Anand is a chess player'.split()\n",
    "\n",
    "# Prepare a dictionary and a corpus.\n",
    "similarity = word2vec_model300.wmdistance(sent_1, sent_2)\n",
    "print(f\"{similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import softcossim\n",
    "from gensim import corpora\n",
    "\n",
    "sent_1 = 'Sachin is a cricket player and a opening batsman'.split()\n",
    "sent_2 = 'Dhoni is a cricket player too He is a batsman and keeper'.split()\n",
    "sent_3 = 'Anand is a chess player'.split()\n",
    "\n",
    "# Prepare a dictionary and a corpus.\n",
    "documents = [sent_1, sent_2, sent_3]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = word2vec_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# Convert the sentences into bag-of-words vectors.\n",
    "sent_1 = dictionary.doc2bow(sent_1)\n",
    "sent_2 = dictionary.doc2bow(sent_2)\n",
    "sent_3 = dictionary.doc2bow(sent_3)\n",
    "\n",
    "# Compute soft cosine similarity\n",
    "print(softcossim(sent_1, sent_2, similarity_matrix))\n",
    "\n",
    "print(softcossim(sent_1, sent_3, similarity_matrix))\n",
    "\n",
    "print(softcossim(sent_2, sent_3, similarity_matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation de son propre word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download dataset\n",
    "dataset = api.load(\"text8\")\n",
    "\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# Split the data into 2 parts. Part 2 will be used later to update the model\n",
    "data_part1 = data[:1000]\n",
    "data_part2 = data[1000:]\n",
    "\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "model = Word2Vec(data_part1, min_count = 1, workers=cpu_count())\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "#print(words)\n",
    "\n",
    "# Save and Load Model\n",
    "model.save('Text8_W2Vec')\n",
    "model = Word2Vec.load('Text8_W2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access vector for one word\n",
    "print(model['sentence'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise à jour d'un W2Vec (on prend la 2nde partie de notre fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model with new data.\n",
    "model.build_vocab(data_part2, update=True)\n",
    "model.train(data_part2, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model['topic']\n",
    "# array([-0.6482, -0.5468,  1.0688,  0.82  , ... , -0.8411,  0.3974], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autres création d'un W2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import os\n",
    "\n",
    "\n",
    "dir_path= 'data/dataN7'\n",
    "\n",
    "for file_name in os.listdir(dir_path):\n",
    "    if \".txt\" in file_name:  \n",
    "        print(file_name)\n",
    "        tokens = [simple_preprocess(sentence) for sentence in open(os.path.join(dir_path, file_name), encoding='utf-8')]\n",
    "\n",
    "#print(tokens)\n",
    "        \n",
    "\n",
    "model = Word2Vec(tokens, min_count = 1, workers=cpu_count())\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "#print(model['sentence'])\n",
    "\n",
    "# access vector for one word\n",
    "print(model['earth'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save('data/dataN7/w2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model en ascii \n",
    "model.wv.save_word2vec_format('data/dataN7/w2vec_model_ascii', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "model = Word2Vec.load('data/dataN7/w2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction d'un fichier TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "# Create gensim dictionary form a single tet file\n",
    "dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('data/dataN7/text_file1.txt', encoding='utf-8'))\n",
    "\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 0\n",
    "size = 50\n",
    "window = 3\n",
    "corpus=[]\n",
    "\n",
    "#construire son propre w2Vec\n",
    "for sentence in texte:\n",
    "    corpus.append(sentence.split())\n",
    "    model = Word2Vec(corpus, min_count=min_count, size=size, window=window)\n",
    "\n",
    "#pprint(model.most_similar('the'))\n",
    "# appeler un W2Vec \n",
    "#print(\"début\")\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#pprint(\"fin\")\n",
    "#pprint(model.most_similar('hello'))\n",
    "#pprint(\"fin2\")                                                     \n",
    "#tokens_s=[]\n",
    "#for token in texte:\n",
    "    # tokens_s.append(stemmer.stem(token))\n",
    "    # tokens_s.append(lemmatizer.lemmatize(token))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
